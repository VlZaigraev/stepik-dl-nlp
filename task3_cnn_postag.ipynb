{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qT4xtHUBdof"
      },
      "source": [
        "# Свёрточные нейросети и POS-теггинг\n",
        "\n",
        "POS-теггинг - определение частей речи (снятие частеречной неоднозначности)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "3_tRuPMaE7tw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3t7dRIZBdok"
      },
      "outputs": [],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        "\n",
        "!git clone https://github.com/VlZaigraev/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:42:57.976431Z",
          "start_time": "2019-10-29T19:42:57.959538Z"
        },
        "id": "H4zPl7QoBdom"
      },
      "outputs": [],
      "source": [
        " !pip install pyconll\n",
        " !pip install spacy_udpipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:34.549739Z",
          "start_time": "2019-10-29T19:49:32.179692Z"
        },
        "id": "UT7MTRWvBdon",
        "outputId": "f8eef2f6-59ee-43c0-ab54-6f041d7b0c7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pyconll\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import dlnlputils\n",
        "from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n",
        "    character_tokenize, pos_corpus_to_tensor, POSTagger\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        "\n",
        "init_random_seed()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iYsAy9iBq1vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5DAEePkBdoo"
      },
      "source": [
        "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:08.433599Z",
          "start_time": "2019-10-29T19:46:05.110693Z"
        },
        "id": "6XDSDs61Bdoo",
        "outputId": "5fb52776-e91f-4213-ee2d-0a3955627aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-15 10:17:13--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40751324 (39M) [text/plain]\n",
            "Saving to: ‘./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu’\n",
            "\n",
            "./stepik-dl-nlp/dat 100%[===================>]  38.86M   162MB/s    in 0.2s    \n",
            "\n",
            "2022-04-15 10:17:17 (162 MB/s) - ‘./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu’ saved [40751324/40751324]\n",
            "\n",
            "--2022-04-15 10:17:17--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14711880 (14M) [text/plain]\n",
            "Saving to: ‘./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "./stepik-dl-nlp/dat 100%[===================>]  14.03M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-04-15 10:17:18 (96.5 MB/s) - ‘./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu’ saved [14711880/14711880]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "!wget -O ./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
        "!wget -O ./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:56.525561Z",
          "start_time": "2019-10-29T19:49:37.315213Z"
        },
        "id": "z_OfqJXSBdop"
      },
      "outputs": [],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "full_train = pyconll.load_from_file('./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu')\n",
        "full_test = pyconll.load_from_file('./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_test"
      ],
      "metadata": {
        "id": "b1kApkELviX1",
        "outputId": "fc4fde8f-535a-4366-9359-8fbd4404d410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyconll.unit.conll.Conll at 0x7f94bb467050>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:56.548127Z",
          "start_time": "2019-10-29T19:49:56.527559Z"
        },
        "id": "q_67q5NkBdoq",
        "outputId": "ab0e3b76-c813-4ad4-9d6c-7cd279499f14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence: Алгоритм , от имени учёного аль - Хорезми , - точный набор инструкций , описывающих порядок действий исполнителя для достижения результата решения задачи за конечное время .\n",
            "Tokens:\n",
            "Алгоритм -> NOUN\n",
            ", -> PUNCT\n",
            "от -> ADP\n",
            "имени -> NOUN\n",
            "учёного -> NOUN\n",
            "аль -> PART\n",
            "- -> PUNCT\n",
            "Хорезми -> PROPN\n",
            ", -> PUNCT\n",
            "- -> PUNCT\n",
            "точный -> ADJ\n",
            "набор -> NOUN\n",
            "инструкций -> NOUN\n",
            ", -> PUNCT\n",
            "описывающих -> VERB\n",
            "порядок -> NOUN\n",
            "действий -> NOUN\n",
            "исполнителя -> NOUN\n",
            "для -> ADP\n",
            "достижения -> NOUN\n",
            "результата -> NOUN\n",
            "решения -> NOUN\n",
            "задачи -> NOUN\n",
            "за -> ADP\n",
            "конечное -> ADJ\n",
            "время -> NOUN\n",
            ". -> PUNCT\n",
            "======================================================\n",
            "\n",
            "Sentence: В старой трактовке вместо слова \" порядок \" использовалось слово \" последовательность \" , но по мере развития параллельности в работе компьютеров слово \" последовательность \" стали заменять более общим словом \" порядок \" .\n",
            "Tokens:\n",
            "В -> ADP\n",
            "старой -> ADJ\n",
            "трактовке -> NOUN\n",
            "вместо -> ADP\n",
            "слова -> NOUN\n",
            "\" -> PUNCT\n",
            "порядок -> NOUN\n",
            "\" -> PUNCT\n",
            "использовалось -> VERB\n",
            "слово -> NOUN\n",
            "\" -> PUNCT\n",
            "последовательность -> NOUN\n",
            "\" -> PUNCT\n",
            ", -> PUNCT\n",
            "но -> CCONJ\n",
            "по -> ADP\n",
            "мере -> NOUN\n",
            "развития -> NOUN\n",
            "параллельности -> NOUN\n",
            "в -> ADP\n",
            "работе -> NOUN\n",
            "компьютеров -> NOUN\n",
            "слово -> NOUN\n",
            "\" -> PUNCT\n",
            "последовательность -> NOUN\n",
            "\" -> PUNCT\n",
            "стали -> VERB\n",
            "заменять -> VERB\n",
            "более -> ADV\n",
            "общим -> ADJ\n",
            "словом -> NOUN\n",
            "\" -> PUNCT\n",
            "порядок -> NOUN\n",
            "\" -> PUNCT\n",
            ". -> PUNCT\n",
            "======================================================\n"
          ]
        }
      ],
      "source": [
        "for sent in full_test[:2]:\n",
        "    print(f\"\"\"\n",
        "Sentence: {' '.join([token.form for token in sent])}\n",
        "Tokens:\"\"\")\n",
        "    for token in sent:\n",
        "        print(f\"{token.form} -> {token.upos}\")\n",
        "    print('='*54)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:56.916262Z",
          "start_time": "2019-10-29T19:49:56.549806Z"
        },
        "id": "qsJIF-fiBdor",
        "outputId": "7eec608c-fd2c-4730-dbfe-60c4adb8a2e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Наибольшая длина предложения 194\n",
            "Наибольшая длина токена 31\n"
          ]
        }
      ],
      "source": [
        "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
        "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
        "\n",
        "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
        "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:57.251433Z",
          "start_time": "2019-10-29T19:49:56.919818Z"
        },
        "id": "07Pn8rrJBdor",
        "outputId": "04f32ad6-d80f-4cff-cb47-b75c6f82d5bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 sentences in train:\n",
            "Анкета .\n",
            "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
            "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
            "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
            "Приемная была обставлена просто , но по-деловому .\n",
            "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
            "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
            "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
            "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
            "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
          ]
        }
      ],
      "source": [
        "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
        "print(\"First 10 sentences in train:\")\n",
        "print('\\n'.join(all_train_texts[:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:58.124148Z",
          "start_time": "2019-10-29T19:49:57.254191Z"
        },
        "id": "WPEE8FlvBdos",
        "outputId": "5886abf7-21af-4209-d045-9894e925ad6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество уникальных символов: 142\n",
            "[('<PAD>', 0), (' ', 1), ('о', 2), ('е', 3), ('а', 4), ('т', 5), ('и', 6), ('н', 7), ('.', 8), ('с', 9)]\n"
          ]
        }
      ],
      "source": [
        "train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n",
        "char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\n",
        "print(f\"Количество уникальных символов: {len(char_vocab)}\")\n",
        "print(list(char_vocab.items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:58.524125Z",
          "start_time": "2019-10-29T19:49:58.125577Z"
        },
        "id": "Zqc7atqoBdos",
        "outputId": "d2a0c38e-1532-41f7-86b0-bdeef5996764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<NOTAG>': 0,\n",
              " 'ADJ': 1,\n",
              " 'ADP': 2,\n",
              " 'ADV': 3,\n",
              " 'AUX': 4,\n",
              " 'CCONJ': 5,\n",
              " 'DET': 6,\n",
              " 'INTJ': 7,\n",
              " 'NOUN': 8,\n",
              " 'NUM': 9,\n",
              " 'PART': 10,\n",
              " 'PRON': 11,\n",
              " 'PROPN': 12,\n",
              " 'PUNCT': 13,\n",
              " 'SCONJ': 14,\n",
              " 'SYM': 15,\n",
              " 'VERB': 16,\n",
              " 'X': 17}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n",
        "label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n",
        "label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:58.752672Z",
          "start_time": "2019-10-29T19:49:58.526431Z"
        },
        "id": "cMy-J5poBdot"
      },
      "outputs": [],
      "source": [
        "train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)"
      ],
      "metadata": {
        "id": "2rVSk_-sLXUB",
        "outputId": "7f3e9c14-1a8d-496d-d604-874e587fc618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-3ad55f580d04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_corpus_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SENT_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_ORIG_TOKEN_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/stepik-dl-nlp/dlnlputils/data/pos.py\u001b[0m in \u001b[0;36mpos_corpus_to_tensor\u001b[0;34m(sentences, char2id, label2id, max_sent_len, max_token_len)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchar_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs.shape, len(train_dataset)"
      ],
      "metadata": {
        "id": "Mi7XWnP4vx52",
        "outputId": "afc7efe1-ecf3-4ca3-abc0-992df61bcc96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([24516, 194, 33]), 24516)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:58.754883Z",
          "start_time": "2019-10-29T19:49:40.582Z"
        },
        "scrolled": true,
        "id": "ScMWgyfxBdot",
        "outputId": "d5d7dd7d-3eae-49dd-9c83-4f9ca42bf483",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "train_inputs[1, 34]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:58.756496Z",
          "start_time": "2019-10-29T19:49:40.711Z"
        },
        "id": "_NaQXoLaBdot",
        "outputId": "a03dd64a-d0b3-471c-da59-13ec33d55314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 8,  1,  8,  8, 12, 12,  4,  8,  1, 13, 16,  2,  8,  3,  3, 13, 16,  2,\n",
              "         8,  2,  8,  5,  3, 10, 16,  2,  8,  8,  2,  8, 13,  8, 13, 13,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "train_labels[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n6iwYTqBdou"
      },
      "source": [
        "## Вспомогательная свёрточная архитектура"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.316516Z",
          "start_time": "2019-10-29T19:46:17.539Z"
        },
        "id": "bevYUE4VBdou"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock1d(nn.Module):\n",
        "    def __init__(self, features_num, layers_num=1, kernel_size=3, dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        # layers = []\n",
        "        # for _ in range(layers_n):\n",
        "        #     layers.append(nn.Sequential(\n",
        "        #         nn.Conv1d(features_num, features_num, kernel_size, padding=kernel_size//2),\n",
        "        #         nn.Dropout(dropout),\n",
        "        #         nn.LeakyReLU()))\n",
        "            \n",
        "        layers = [nn.Sequential(\n",
        "            nn.Conv1d(in_channels = features_num,\n",
        "                      out_channels = features_num,\n",
        "                      kernel_size = kernel_size,\n",
        "                      padding=kernel_size//2),\n",
        "            nn.Dropout(p = dropout_rate),\n",
        "            nn.LeakyReLU()\n",
        "        ) for _ in range(layers_num)]\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = x + layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJIgW01SBdou"
      },
      "source": [
        "## Предсказание частей речи на уровне отдельных токенов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.317452Z",
          "start_time": "2019-10-29T19:46:23.135Z"
        },
        "id": "0qGgYRJgBdov"
      },
      "outputs": [],
      "source": [
        "class SingleTokenPOSTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
        "        super().__init__()\n",
        "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "        self.backbone = ResidualBlock1d(embedding_size, **kwargs)\n",
        "        self.global_pooling = nn.AdaptiveMaxPool1d(output_size = 1)\n",
        "        self.out = nn.Linear(embedding_size, labels_num)\n",
        "        self.labels_num = labels_num\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
        "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
        "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
        "        \n",
        "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
        "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
        "        \n",
        "        features = self.backbone(char_embeddings)\n",
        "        \n",
        "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
        "        \n",
        "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
        "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n",
        "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.318497Z",
          "start_time": "2019-10-29T19:46:23.764Z"
        },
        "id": "OAlbTk2YBdov",
        "outputId": "0d31a89f-56af-460e-b5a7-4b46c142df34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество параметров: 47314 \n",
            "\n",
            "SingleTokenPOSTagger(\n",
            "  (char_embeddings): Embedding(142, 64, padding_idx=0)\n",
            "  (backbone): ResidualBlock1d(\n",
            "    (layers): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Dropout(p=0.3, inplace=False)\n",
            "        (2): LeakyReLU(negative_slope=0.01)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Dropout(p=0.3, inplace=False)\n",
            "        (2): LeakyReLU(negative_slope=0.01)\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Dropout(p=0.3, inplace=False)\n",
            "        (2): LeakyReLU(negative_slope=0.01)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (global_pooling): AdaptiveMaxPool1d(output_size=1)\n",
            "  (out): Linear(in_features=64, out_features=18, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(char_vocab)\n",
        "labels_num = len(label2id)\n",
        "\n",
        "res_block_kwargs = {'layers_num':3,\n",
        "                    'kernel_size':3,\n",
        "                    'dropout_rate':0.3}\n",
        "\n",
        "single_token_model = SingleTokenPOSTagger(vocab_size, labels_num, embedding_size=64, **res_block_kwargs)\n",
        "print('Количество параметров:', sum(np.product(t.shape) for t in single_token_model.parameters()), '\\n')\n",
        "print(single_token_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.319470Z",
          "start_time": "2019-10-29T19:46:25.552Z"
        },
        "scrolled": false,
        "id": "mXIsfq3NBdov"
      },
      "outputs": [],
      "source": [
        "(best_val_loss,\n",
        " best_single_token_model) = train_eval_loop(single_token_model,\n",
        "                                            train_dataset,\n",
        "                                            test_dataset,\n",
        "                                            F.cross_entropy,\n",
        "                                            lr=5e-3,\n",
        "                                            epoch_n=10,\n",
        "                                            batch_size=64,\n",
        "                                            device='cuda',\n",
        "                                            early_stopping_patience=5,\n",
        "                                            max_batches_per_epoch_train=500,\n",
        "                                            max_batches_per_epoch_val=100,\n",
        "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
        "                                                                                                                       factor=0.5,\n",
        "                                                                                                                       verbose=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.320568Z",
          "start_time": "2019-10-29T19:46:47.579Z"
        },
        "id": "SFpgA4Y4Bdow"
      },
      "outputs": [],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "torch.save(best_single_token_model.state_dict(), './models/single_token_pos.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.321566Z",
          "start_time": "2019-10-29T19:46:47.731Z"
        },
        "id": "ZhBqYsoABdow"
      },
      "outputs": [],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "single_token_model.load_state_dict(torch.load('./models/single_token_pos.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.324276Z",
          "start_time": "2019-10-29T19:46:48.445Z"
        },
        "id": "8Nvd4HDjBdow"
      },
      "outputs": [],
      "source": [
        "train_pred = predict_with_model(single_token_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
        "                             torch.tensor(train_labels))\n",
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
        "print()\n",
        "\n",
        "test_pred = predict_with_model(single_token_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
        "                            torch.tensor(test_labels))\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPgHQDTSBdox"
      },
      "source": [
        "## Предсказание частей речи на уровне предложений (с учётом контекста)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.325744Z",
          "start_time": "2019-10-29T19:46:50.139Z"
        },
        "id": "Yxk3WEyiBdox"
      },
      "outputs": [],
      "source": [
        "class SentenceLevelPOSTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
        "        super().__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
        "        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
        "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
        "        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
        "        self.labels_num = labels_num\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
        "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
        "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
        "        \n",
        "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
        "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
        "        char_features = self.single_token_backbone(char_embeddings)\n",
        "        \n",
        "        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
        "\n",
        "        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n",
        "        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n",
        "        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n",
        "\n",
        "        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.326925Z",
          "start_time": "2019-10-29T19:46:50.310Z"
        },
        "id": "QhzndMBMBdox"
      },
      "outputs": [],
      "source": [
        "sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
        "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
        "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\n",
        "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.327888Z",
          "start_time": "2019-10-29T19:46:50.737Z"
        },
        "scrolled": false,
        "id": "4x8xvi8ZBdox"
      },
      "outputs": [],
      "source": [
        "(best_val_loss,\n",
        " best_sentence_level_model) = train_eval_loop(sentence_level_model,\n",
        "                                              train_dataset,\n",
        "                                              test_dataset,\n",
        "                                              F.cross_entropy,\n",
        "                                              lr=5e-3,\n",
        "                                              epoch_n=10,\n",
        "                                              batch_size=64,\n",
        "                                              device='cuda',\n",
        "                                              early_stopping_patience=5,\n",
        "                                              max_batches_per_epoch_train=500,\n",
        "                                              max_batches_per_epoch_val=100,\n",
        "                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
        "                                                                                                                         factor=0.5,\n",
        "                                                                                                                         verbose=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:16.542052Z",
          "start_time": "2019-08-29T13:56:16.529110Z"
        },
        "id": "4W5z93atBdoy"
      },
      "outputs": [],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "torch.save(best_sentence_level_model.state_dict(), './models/sentence_level_pos.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:16.564926Z",
          "start_time": "2019-08-29T13:56:16.544481Z"
        },
        "id": "dP_fTr9pBdoy"
      },
      "outputs": [],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "sentence_level_model.load_state_dict(torch.load('./models/sentence_level_pos.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.092139Z",
          "start_time": "2019-08-29T13:56:16.567242Z"
        },
        "id": "UlzmvvsjBdoy"
      },
      "outputs": [],
      "source": [
        "train_pred = predict_with_model(sentence_level_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
        "                             torch.tensor(train_labels))\n",
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
        "print()\n",
        "\n",
        "test_pred = predict_with_model(sentence_level_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
        "                            torch.tensor(test_labels))\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8iwx3qLBdoz"
      },
      "source": [
        "## Применение полученных теггеров и сравнение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.105418Z",
          "start_time": "2019-08-29T13:56:42.093744Z"
        },
        "id": "VDt1AtW1Bdoz"
      },
      "outputs": [],
      "source": [
        "single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
        "sentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.125540Z",
          "start_time": "2019-08-29T13:56:42.106771Z"
        },
        "id": "OylvMFd8Bdoz"
      },
      "outputs": [],
      "source": [
        "test_sentences = [\n",
        "    'Мама мыла раму.',\n",
        "    'Косил косой косой косой.',\n",
        "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
        "    'Сяпала Калуша с Калушатами по напушке.',\n",
        "    'Пирожки поставлены в печь, мама любит печь.',\n",
        "    'Ведро дало течь, вода стала течь.',\n",
        "    'Три да три, будет дырка.',\n",
        "    'Три да три, будет шесть.',\n",
        "    'Сорок сорок'\n",
        "]\n",
        "test_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.148124Z",
          "start_time": "2019-08-29T13:56:42.126930Z"
        },
        "id": "RUgU7UniBdo0"
      },
      "outputs": [],
      "source": [
        "for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n",
        "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.168810Z",
          "start_time": "2019-08-29T13:56:42.149698Z"
        },
        "id": "YAVR46KbBdo0"
      },
      "outputs": [],
      "source": [
        "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n",
        "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61UNbZfhBdo0"
      },
      "source": [
        "## Свёрточный модуль своими руками"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.193140Z",
          "start_time": "2019-08-29T13:56:42.170233Z"
        },
        "id": "RRrVZgC0Bdo0"
      },
      "outputs": [],
      "source": [
        "class MyConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n",
        "                                   requires_grad=True)\n",
        "        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n",
        "\n",
        "        batch_size, src_channels, sequence_len = x.shape        \n",
        "        if self.padding > 0:\n",
        "            pad = x.new_zeros(batch_size, src_channels, self.padding)\n",
        "            x = torch.cat((pad, x, pad), dim=-1)\n",
        "            sequence_len = x.shape[-1]\n",
        "\n",
        "        chunks = []\n",
        "        chunk_size = sequence_len - self.kernel_size + 1\n",
        "        for offset in range(self.kernel_size):\n",
        "            chunks.append(x[:, :, offset:offset + chunk_size])\n",
        "\n",
        "        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n",
        "        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n",
        "        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n",
        "        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n",
        "        return out_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.210013Z",
          "start_time": "2019-08-29T13:56:42.194620Z"
        },
        "id": "rpWS9h_8Bdo1"
      },
      "outputs": [],
      "source": [
        "sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
        "                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n",
        "                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\n",
        "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T14:06:00.233326Z",
          "start_time": "2019-08-29T13:56:42.211456Z"
        },
        "id": "g_7ILb04Bdo1"
      },
      "outputs": [],
      "source": [
        "(best_val_loss,\n",
        " best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n",
        "                                                      train_dataset,\n",
        "                                                      test_dataset,\n",
        "                                                      F.cross_entropy,\n",
        "                                                      lr=5e-3,\n",
        "                                                      epoch_n=10,\n",
        "                                                      batch_size=64,\n",
        "                                                      device='cuda',\n",
        "                                                      early_stopping_patience=5,\n",
        "                                                      max_batches_per_epoch_train=500,\n",
        "                                                      max_batches_per_epoch_val=100,\n",
        "                                                      lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
        "                                                                                                                                 factor=0.5,\n",
        "                                                                                                                                 verbose=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T14:06:39.145214Z",
          "start_time": "2019-08-29T14:06:00.234936Z"
        },
        "id": "G0Nr5nauBdo1"
      },
      "outputs": [],
      "source": [
        "train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
        "                             torch.tensor(train_labels))\n",
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
        "print()\n",
        "\n",
        "test_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
        "                            torch.tensor(test_labels))\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "task3_cnn_postag.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}